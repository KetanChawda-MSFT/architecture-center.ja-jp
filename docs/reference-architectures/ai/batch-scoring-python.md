---
title: Azure での Python モデルのバッチ スコアリング
description: Azure Batch AI を使用して、スケジュールに従って複数のモデルのバッチ スコアリングを並列で実行するスケーラブルなソリューションをビルドします。
author: njray
ms.date: 12/13/2018
ms.topic: reference-architecture
ms.service: architecture-center
ms.subservice: reference-architecture
ms.custom: azcat-ai, AI
ms.openlocfilehash: a291821860a8e503ba4c6173ac6d8fd449d6ebf3
ms.sourcegitcommit: 1b50810208354577b00e89e5c031b774b02736e2
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 01/23/2019
ms.locfileid: "54485368"
---
# <a name="batch-scoring-of-python-models-on-azure"></a><span data-ttu-id="e4912-103">Azure での Python モデルのバッチ スコアリング</span><span class="sxs-lookup"><span data-stu-id="e4912-103">Batch scoring of Python models on Azure</span></span>

<span data-ttu-id="e4912-104">この参照アーキテクチャでは、Azure Batch AI を使用して、スケジュールに従って複数のモデルのバッチ スコアリングを並列で実行するスケーラブルなソリューションをビルドする方法を示します。</span><span class="sxs-lookup"><span data-stu-id="e4912-104">This reference architecture shows how to build a scalable solution for batch scoring many models on a schedule in parallel using Azure Batch AI.</span></span> <span data-ttu-id="e4912-105">このソリューションはテンプレートとして使用でき、さまざまな問題に対応するように汎用化できます。</span><span class="sxs-lookup"><span data-stu-id="e4912-105">The solution can be used as a template and can generalize to different problems.</span></span>

<span data-ttu-id="e4912-106">このアーキテクチャのリファレンス実装は、 [GitHub][github] で入手できます。</span><span class="sxs-lookup"><span data-stu-id="e4912-106">A reference implementation for this architecture is available on [GitHub][github].</span></span>

![Azure での Python モデルのバッチ スコアリング](./_images/batch-scoring-python.png)

<span data-ttu-id="e4912-108">**シナリオ**: このソリューションでは、各デバイスがセンサーの測定値を継続的に送信する IoT 設定内の多数のデバイスの運用を監視します。</span><span class="sxs-lookup"><span data-stu-id="e4912-108">**Scenario**: This solution monitors the operation of a large number of devices in an IoT setting where each device sends sensor readings continuously.</span></span> <span data-ttu-id="e4912-109">各デバイスには、定義済みの期間にわたって集計される一連の測定値が異常に該当するか否かを予測するために使用する必要がある、トレーニング済みの異常検出モデルが存在することを前提としています。</span><span class="sxs-lookup"><span data-stu-id="e4912-109">Each device is assumed to have pre-trained anomaly detection models that need to be used to predict whether a series of measurements, that are aggregated over a predefined time interval, correspond to an anomaly or not.</span></span> <span data-ttu-id="e4912-110">現実のシナリオでは、これは、トレーニングやリアルタイム スコアリングで使用する前に、フィルター処理や集計を行う必要があるセンサーの測定値のストリームが考えられます。</span><span class="sxs-lookup"><span data-stu-id="e4912-110">In real-world scenarios, this could be a stream of sensor readings that need to be filtered and aggregated before being used in training or real-time scoring.</span></span> <span data-ttu-id="e4912-111">単純化するために、このソリューションでは、スコアリング ジョブを実行するときに同じデータ ファイルを使用します。</span><span class="sxs-lookup"><span data-stu-id="e4912-111">For simplicity, the solution uses the same data file when executing scoring jobs.</span></span>

## <a name="architecture"></a><span data-ttu-id="e4912-112">アーキテクチャ</span><span class="sxs-lookup"><span data-stu-id="e4912-112">Architecture</span></span>

<span data-ttu-id="e4912-113">このアーキテクチャは、次のコンポーネントで構成されます。</span><span class="sxs-lookup"><span data-stu-id="e4912-113">This architecture consists of the following components:</span></span>

<span data-ttu-id="e4912-114">[Azure Event Hubs][event-hubs]。</span><span class="sxs-lookup"><span data-stu-id="e4912-114">[Azure Event Hubs][event-hubs].</span></span> <span data-ttu-id="e4912-115">このメッセージ インジェスト サービスでは、1 秒あたり数百万件のイベント メッセージを取り込むことができます。</span><span class="sxs-lookup"><span data-stu-id="e4912-115">This message ingestion service can ingest millions of event messages per second.</span></span> <span data-ttu-id="e4912-116">このアーキテクチャでは、センサーがこのイベント ハブにデータ ストリームを送信します。</span><span class="sxs-lookup"><span data-stu-id="e4912-116">In this architecture, sensors send a stream of data to the event hub.</span></span>

<span data-ttu-id="e4912-117">[Azure Stream Analytics][stream-analytics]。</span><span class="sxs-lookup"><span data-stu-id="e4912-117">[Azure Stream Analytics][stream-analytics].</span></span> <span data-ttu-id="e4912-118">イベント処理エンジンです。</span><span class="sxs-lookup"><span data-stu-id="e4912-118">An event-processing engine.</span></span> <span data-ttu-id="e4912-119">Stream Analytics ジョブがイベント ハブからデータ ストリームを読み取り、ストリーム処理を実行します。</span><span class="sxs-lookup"><span data-stu-id="e4912-119">A Stream Analytics job reads the data streams from the event hub and performs stream processing.</span></span>

<span data-ttu-id="e4912-120">[Azure Batch AI][batch-ai]。</span><span class="sxs-lookup"><span data-stu-id="e4912-120">[Azure Batch AI][batch-ai].</span></span> <span data-ttu-id="e4912-121">この分散コンピューティング エンジンを使用して、Azure 上で機械学習モデルと AI モデルの大規模なトレーニングとテストを行います。</span><span class="sxs-lookup"><span data-stu-id="e4912-121">This distributed computing engine is used to train and test machine learning and AI models at scale in Azure.</span></span> <span data-ttu-id="e4912-122">Batch AI は自動スケーリング オプションを使用してオンデマンドで仮想マシンを作成し、Batch AI クラスター内の各ノードで、特定のセンサーに対するスコアリング ジョブが実行されます。</span><span class="sxs-lookup"><span data-stu-id="e4912-122">Batch AI creates virtual machines on demand with an automatic scaling option, where each node in the Batch AI cluster runs a scoring job for a specific sensor.</span></span> <span data-ttu-id="e4912-123">スコアリング Python [スクリプト][python-script]は、クラスターの各ノードに作成される Docker コンテナーで実行され、関連するセンサーのデータを読み取り、予測を生成して Blob Storage に格納します。</span><span class="sxs-lookup"><span data-stu-id="e4912-123">The scoring Python [script][python-script] runs in Docker containers that are created on each node of the cluster, where it reads the relevant sensor data, generates predictions and stores them in Blob storage.</span></span>

<span data-ttu-id="e4912-124">[Azure Blob Storage][storage]。</span><span class="sxs-lookup"><span data-stu-id="e4912-124">[Azure Blob Storage][storage].</span></span> <span data-ttu-id="e4912-125">BLOB コンテナーを使用して、事前トレーニング済みモデル、データ、および出力予測が格納されます。</span><span class="sxs-lookup"><span data-stu-id="e4912-125">Blob containers are used to store the pretrained models, the data, and the output predictions.</span></span> <span data-ttu-id="e4912-126">モデルは、Blob Storage の [create\_resources.ipynb][create-resources] ノートブックにアップロードされます。</span><span class="sxs-lookup"><span data-stu-id="e4912-126">The models are uploaded to Blob storage in the [create\_resources.ipynb][create-resources] notebook.</span></span> <span data-ttu-id="e4912-127">これらの [1 クラス SVM][one-class-svm] モデルが、異なるデバイスの異なるセンサーの値を表すデータでトレーニングされます。</span><span class="sxs-lookup"><span data-stu-id="e4912-127">These [one-class SVM][one-class-svm] models are trained on data that represents values of different sensors for different devices.</span></span> <span data-ttu-id="e4912-128">このソリューションでは、固定された期間にわたってデータ値が集計されることを前提としています。</span><span class="sxs-lookup"><span data-stu-id="e4912-128">This solution assumes that the data values are aggregated over a fixed interval of time.</span></span>

<span data-ttu-id="e4912-129">[Azure Logic Apps][logic-apps]。</span><span class="sxs-lookup"><span data-stu-id="e4912-129">[Azure Logic Apps][logic-apps].</span></span> <span data-ttu-id="e4912-130">このソリューションでは、1 時間ごとに Batch AI ジョブを実行するロジック アプリを作成します。</span><span class="sxs-lookup"><span data-stu-id="e4912-130">This solution creates a Logic App that runs hourly Batch AI jobs.</span></span> <span data-ttu-id="e4912-131">Logic Apps には、ソリューションのランタイム ワークフローとスケジュールを簡単に作成する方法が用意されています。</span><span class="sxs-lookup"><span data-stu-id="e4912-131">Logic Apps provides an easy way to create the runtime workflow and scheduling for the solution.</span></span> <span data-ttu-id="e4912-132">同じように Docker コンテナーで実行される Python [スクリプト][script]を使用して、Batch AI ジョブが送信されます。</span><span class="sxs-lookup"><span data-stu-id="e4912-132">The Batch AI jobs are submitted using a Python [script][script] that also runs in a Docker container.</span></span>

<span data-ttu-id="e4912-133">[Azure Container Registry][acr]。</span><span class="sxs-lookup"><span data-stu-id="e4912-133">[Azure Container Registry][acr].</span></span> <span data-ttu-id="e4912-134">Docker イメージは Batch AI と Logic Apps の両方で使用され、[create\_resources.ipynb][create-resources] ノートブック内に作成された後、Container Registry にプッシュされます。</span><span class="sxs-lookup"><span data-stu-id="e4912-134">Docker images are used in both Batch AI and Logic Apps and are created in the [create\_resources.ipynb][create-resources] notebook, then pushed to Container Registry.</span></span> <span data-ttu-id="e4912-135">これにより、他の Azure サービス (このソリューションでは Logic Apps と Batch AI) を使用してイメージのホストとコンテナーのインスタンス化を行う便利な方法が提供されます。</span><span class="sxs-lookup"><span data-stu-id="e4912-135">This provides a convenient way to host images and instantiate containers through other Azure services—Logic Apps and Batch AI in this solution.</span></span>

## <a name="performance-considerations"></a><span data-ttu-id="e4912-136">パフォーマンスに関する考慮事項</span><span class="sxs-lookup"><span data-stu-id="e4912-136">Performance considerations</span></span>

<span data-ttu-id="e4912-137">標準的な Python モデルでは、CPU で十分にワークロードを処理できることが一般に認められています。</span><span class="sxs-lookup"><span data-stu-id="e4912-137">For standard Python models, it's generally accepted that CPUs are sufficient to handle the workload.</span></span> <span data-ttu-id="e4912-138">このアーキテクチャでは、CPU を使用します。</span><span class="sxs-lookup"><span data-stu-id="e4912-138">This architecture uses CPUs.</span></span> <span data-ttu-id="e4912-139">ただし、[ディープ ラーニング ワークロード][deep]では、通常は、CPU より GPU の方が優れています。多くの場合、同等のパフォーマンスを得るためには巨大な CPU クラスターが必要です。</span><span class="sxs-lookup"><span data-stu-id="e4912-139">However, for [deep learning workloads][deep], GPUs generally outperform CPUs by a considerable amount—a sizeable cluster of CPUs is usually needed to get comparable performance.</span></span>

### <a name="parallelizing-across-vms-vs-cores"></a><span data-ttu-id="e4912-140">VM とコアの間の並列化</span><span class="sxs-lookup"><span data-stu-id="e4912-140">Parallelizing across VMs vs cores</span></span>

<span data-ttu-id="e4912-141">多数のモデルのスコアリング プロセスをバッチ モードで実行する場合は、VM 間でジョブを並列処理する必要があります。</span><span class="sxs-lookup"><span data-stu-id="e4912-141">When running scoring processes of many models in batch mode, the jobs need to be parallelized across VMs.</span></span> <span data-ttu-id="e4912-142">2 つの方法が可能であり、</span><span class="sxs-lookup"><span data-stu-id="e4912-142">Two approaches are possible:</span></span>

* <span data-ttu-id="e4912-143">低コストの VM を使用して大規模なクラスターを作成する。</span><span class="sxs-lookup"><span data-stu-id="e4912-143">Create a larger cluster using low-cost VMs.</span></span>

* <span data-ttu-id="e4912-144">高パフォーマンスの VM を使用する小規模なクラスターを作成し、それぞれで複数のコアを使用できるようにする。</span><span class="sxs-lookup"><span data-stu-id="e4912-144">Create a smaller cluster using high performing VMs with more cores available on each.</span></span>

<span data-ttu-id="e4912-145">一般に、標準的な Python モデルのスコアリングはディープ ラーニング モデルのスコアリングほど負荷が高くないため、小規模のクラスターでキューに置かれた多数のモデルを効率的に処理できます。</span><span class="sxs-lookup"><span data-stu-id="e4912-145">In general, scoring of standard Python models is not as demanding as scoring of deep learning models, and a small cluster should be able to handle a large number of queued models efficiently.</span></span> <span data-ttu-id="e4912-146">データセットのサイズが大きくなったときに、クラスター ノードの数を増やすことができます。</span><span class="sxs-lookup"><span data-stu-id="e4912-146">You can increase the number of cluster nodes as the dataset sizes increase.</span></span>

<span data-ttu-id="e4912-147">このシナリオでは、便宜上、単一の Batch AI ジョブ内で 1 つのスコアリング タスクを送信します。</span><span class="sxs-lookup"><span data-stu-id="e4912-147">For convenience in this scenario, one scoring task is submitted within a single Batch AI job.</span></span> <span data-ttu-id="e4912-148">ただし、同じ Batch AI ジョブ内で複数のデータ チャンクをスコアリングすることで、効率を上げることができます。</span><span class="sxs-lookup"><span data-stu-id="e4912-148">However, it can be more efficient to score multiple data chunks within the same Batch AI job.</span></span> <span data-ttu-id="e4912-149">この場合は、単一の Batch AI ジョブの実行中に、複数のデータセットを読み取り、それらに対してスコアリング スクリプトを実行するカスタム コードを記述します。</span><span class="sxs-lookup"><span data-stu-id="e4912-149">In those cases, write custom code to read in multiple datasets and execute the scoring script for those during a single Batch AI job execution.</span></span>

### <a name="file-servers"></a><span data-ttu-id="e4912-150">ファイル サーバー</span><span class="sxs-lookup"><span data-stu-id="e4912-150">File servers</span></span>

<span data-ttu-id="e4912-151">Batch AI を使用する場合は、シナリオに必要なスループットに応じて、複数のストレージ オプションを選択できます。</span><span class="sxs-lookup"><span data-stu-id="e4912-151">When using Batch AI, you can choose multiple storage options depending on the throughput needed for your scenario.</span></span> <span data-ttu-id="e4912-152">スループットの要件が低いワークロードでは、Blob ストレージを使用すれば十分です。</span><span class="sxs-lookup"><span data-stu-id="e4912-152">For workloads with low throughput requirements, using blob storage should be enough.</span></span> <span data-ttu-id="e4912-153">また、Batch AI では、管理された単一ノード NFS である [Batch AI ファイル サーバー][bai-file-server]もサポートされています。これをクラスター ノードに自動的にマウントして、ジョブに対して一元的にアクセス可能な保存場所を提供できます。</span><span class="sxs-lookup"><span data-stu-id="e4912-153">Alternatively, Batch AI also supports a [Batch AI File Server][bai-file-server], a managed, single-node NFS, which can be automatically mounted on cluster nodes to provide a centrally accessible storage location for jobs.</span></span> <span data-ttu-id="e4912-154">ほとんどの場合、ワークスペースで必要なファイル サーバーは 1 つだけであり、トレーニング ジョブのデータを異なるディレクトリに分離することができます。</span><span class="sxs-lookup"><span data-stu-id="e4912-154">For most cases, only one file server is needed in a workspace, and you can separate data for your training jobs into different directories.</span></span>

<span data-ttu-id="e4912-155">単一ノード NFS がワークロードに適していない場合、Batch AI では、[Azure Files][azure-files] やカスタム ソリューション (Gluster や Lustre ファイル システムなど) を含む他のストレージ オプションもサポートしています。</span><span class="sxs-lookup"><span data-stu-id="e4912-155">If a single-node NFS isn't appropriate for your workloads, Batch AI supports other storage options, including [Azure Files][azure-files] and custom solutions such as a Gluster or Lustre file system.</span></span>

## <a name="management-considerations"></a><span data-ttu-id="e4912-156">管理の考慮事項</span><span class="sxs-lookup"><span data-stu-id="e4912-156">Management considerations</span></span>

### <a name="monitoring-batch-ai-jobs"></a><span data-ttu-id="e4912-157">Batch AI ジョブの監視</span><span class="sxs-lookup"><span data-stu-id="e4912-157">Monitoring Batch AI jobs</span></span>

<span data-ttu-id="e4912-158">実行中のジョブの進行状況を監視することは重要ですが、アクティブ ノードのクラスターを監視することは困難である可能性があります。</span><span class="sxs-lookup"><span data-stu-id="e4912-158">It's important to monitor the progress of running jobs, but it can be a challenge to monitor across a cluster of active nodes.</span></span> <span data-ttu-id="e4912-159">クラスターの全体的な状態を把握するには、[Azure Portal][portal] の **[Batch AI]** ブレードに移動して、クラスター内のノードの状態を調べます。</span><span class="sxs-lookup"><span data-stu-id="e4912-159">To get a sense of the overall state of the cluster, go to the **Batch AI** blade of the [Azure Portal][portal] to inspect the state of the nodes in the cluster.</span></span> <span data-ttu-id="e4912-160">ノードが非アクティブになった場合、またはジョブが失敗した場合は、エラー ログが Blob ストレージに保存され、ポータルの **[ジョブ]** ブレードでもアクセスできます。</span><span class="sxs-lookup"><span data-stu-id="e4912-160">If a node is inactive or a job has failed, the error logs are saved to blob storage, and are also accessible in the **Jobs** blade of the portal.</span></span>

<span data-ttu-id="e4912-161">監視を強化するには、ログを [Application Insights][ai] に接続するか、Batch AI クラスターとそのジョブの状態をポーリングする別のプロセスを実行します。</span><span class="sxs-lookup"><span data-stu-id="e4912-161">For richer monitoring, connect logs to [Application Insights][ai], or run separate processes to poll for the state of the Batch AI cluster and its jobs.</span></span>

### <a name="logging-in-batch-ai"></a><span data-ttu-id="e4912-162">Batch AI でのログ</span><span class="sxs-lookup"><span data-stu-id="e4912-162">Logging in Batch AI</span></span>

<span data-ttu-id="e4912-163">Batch AI では、関連する Azure ストレージ アカウントにすべての stdout/stderr が記録されます。</span><span class="sxs-lookup"><span data-stu-id="e4912-163">Batch AI logs all stdout/stderr to the associated Azure storage account.</span></span> <span data-ttu-id="e4912-164">ログ ファイルを簡単にナビゲートするには、[Azure Storage Explorer][explorer] などのストレージ ナビゲーション ツールを使用します。</span><span class="sxs-lookup"><span data-stu-id="e4912-164">For easy navigation of the log files, use a storage navigation tool such as [Azure Storage Explorer][explorer].</span></span>

<span data-ttu-id="e4912-165">この参照アーキテクチャをデプロイするときに、シンプルなログ記録システムを設定するというオプションがあります。</span><span class="sxs-lookup"><span data-stu-id="e4912-165">When you deploy this reference architecture, you have the option to set up a simpler logging system.</span></span> <span data-ttu-id="e4912-166">このオプションを使用すると、次に示すように、異なるジョブのすべてのログが BLOB コンテナー内の同じディレクトリに保存されます。</span><span class="sxs-lookup"><span data-stu-id="e4912-166">With this option, all the logs across the different jobs are saved to the same directory in your blob container as shown below.</span></span> <span data-ttu-id="e4912-167">これらのログを使用して、各ジョブと各イメージを処理するのにかかる時間を監視することで、プロセスを最適化する方法を理解できます。</span><span class="sxs-lookup"><span data-stu-id="e4912-167">Use these logs to monitor how long it takes for each job and each image to process, so you have a better sense of how to optimize the process.</span></span>

![Azure ストレージ エクスプローラー](./_images/batch-scoring-python-monitor.png)

## <a name="cost-considerations"></a><span data-ttu-id="e4912-169">コストに関する考慮事項</span><span class="sxs-lookup"><span data-stu-id="e4912-169">Cost considerations</span></span>

<span data-ttu-id="e4912-170">この参照アーキテクチャで使用される最も高価なコンポーネントは、コンピューティング リソースです。</span><span class="sxs-lookup"><span data-stu-id="e4912-170">The most expensive components used in this reference architecture are the compute resources.</span></span>

<span data-ttu-id="e4912-171">Batch AI クラスターのサイズは、キュー内のジョブに応じて、スケールアップおよびスケールダウンされます。</span><span class="sxs-lookup"><span data-stu-id="e4912-171">The Batch AI cluster size scales up and down depending on the jobs in the queue.</span></span> <span data-ttu-id="e4912-172">2 つの方法のいずれかで、Batch AI を使用した[自動スケーリング][automatic-scaling]を有効にできます。</span><span class="sxs-lookup"><span data-stu-id="e4912-172">You can enable [automatic scaling][automatic-scaling] with Batch AI in one of two ways.</span></span> <span data-ttu-id="e4912-173">プログラムで行う場合は、[デプロイ手順][github]の一部である .env ファイル内に構成できます。クラスターの作成後に、スケーリング式をポータルで直接変更することもできます。</span><span class="sxs-lookup"><span data-stu-id="e4912-173">You can do so programmatically, which can be configured in the .env file that is part of the [deployment steps][github], or you can change the scale formula directly in the portal after the cluster is created.</span></span>

<span data-ttu-id="e4912-174">即時処理を必要としない作業の場合は、既定の状態 (最小) が 0 個のノードのクラスターになるように、自動スケーリング式を構成します。</span><span class="sxs-lookup"><span data-stu-id="e4912-174">For work that doesn't require immediate processing, configure the automatic scaling formula so the default state (minimum) is a cluster of zero nodes.</span></span> <span data-ttu-id="e4912-175">この構成では、クラスターは 0 個のノードで開始し、キュー内でジョブが検出されたときのみスケールアップします。</span><span class="sxs-lookup"><span data-stu-id="e4912-175">With this configuration, the cluster starts with zero nodes and only scales up when it detects jobs in the queue.</span></span> <span data-ttu-id="e4912-176">バッチ スコアリング プロセスが 1 日に数回以下しか発生しない場合は、この設定により大幅なコスト削減を実現できます。</span><span class="sxs-lookup"><span data-stu-id="e4912-176">If the batch scoring process only happens a few times a day or less, this setting enables significant cost savings.</span></span>

<span data-ttu-id="e4912-177">非常に短い間隔で発生するバッチ ジョブでは、自動スケーリングは適切ではない場合があります。</span><span class="sxs-lookup"><span data-stu-id="e4912-177">Automatic scaling may not be appropriate for batch jobs that happen too close to each other.</span></span> <span data-ttu-id="e4912-178">クラスターの起動と停止に要する時間にもコストがかかるので、前のジョブの終了後ほんの数分でバッチ ワークロードが開始する場合は、ジョブ間もクラスターを実行したままにする方がコスト効率がよくなる可能性があります。</span><span class="sxs-lookup"><span data-stu-id="e4912-178">The time that it takes for a cluster to spin up and spin down also incur a cost, so if a batch workload begins only a few minutes after the previous job ends, it might be more cost effective to keep the cluster running between jobs.</span></span> <span data-ttu-id="e4912-179">これは、スコアリング プロセスが高い頻度で (たとえば 1 時間ごとに) 実行されるようにスケジュールされるか、低い頻度で (たとえば 1 か月に 1 回) 実行されるようにスケジュールされるかによって決まります。</span><span class="sxs-lookup"><span data-stu-id="e4912-179">That depends on whether scoring processes are scheduled to run at a high frequency (every hour, for example), or less frequently (once a month, for example).</span></span>

## <a name="deploy-the-solution"></a><span data-ttu-id="e4912-180">ソリューションのデプロイ方法</span><span class="sxs-lookup"><span data-stu-id="e4912-180">Deploy the solution</span></span>

<span data-ttu-id="e4912-181">このアーキテクチャのリファレンス実装は、[GitHub][github] で入手できます。</span><span class="sxs-lookup"><span data-stu-id="e4912-181">The reference implementation of this architecture is available on [GitHub][github].</span></span> <span data-ttu-id="e4912-182">記載されているセットアップ手順に従って、Batch AI を使用して多数のモデルを並列でスコアリングするスケーラブルなソリューションをビルドします。</span><span class="sxs-lookup"><span data-stu-id="e4912-182">Follow the setup steps there to build a scalable solution for scoring many models in parallel using Batch AI.</span></span>

[acr]: /azure/container-registry/container-registry-intro
[ai]: /azure/application-insights/app-insights-overview
[automatic-scaling]: /azure/batch/batch-automatic-scaling
[azure-files]: /azure/storage/files/storage-files-introduction
[batch-ai]: /azure/batch-ai/
[bai-file-server]: /azure/batch-ai/resource-concepts#file-server
[create-resources]: https://github.com/Azure/BatchAIAnomalyDetection/blob/master/create_resources.ipynb
[deep]: /azure/architecture/reference-architectures/ai/batch-scoring-deep-learning
[event-hubs]: /azure/event-hubs/event-hubs-geo-dr
[explorer]: https://azure.microsoft.com/en-us/features/storage-explorer/
[github]: https://github.com/Azure/BatchAIAnomalyDetection
[logic-apps]: /azure/logic-apps/logic-apps-overview
[one-class-svm]: http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html
[portal]: https://portal.azure.com
[python-script]: https://github.com/Azure/BatchAIAnomalyDetection/blob/master/batchai/predict.py
[script]: https://github.com/Azure/BatchAIAnomalyDetection/blob/master/sched/submit_jobs.py
[storage]: /azure/storage/blobs/storage-blobs-overview
[stream-analytics]: /azure/stream-analytics/
